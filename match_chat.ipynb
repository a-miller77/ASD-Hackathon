{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install git+https://github.com/huggingface/transformers #need to install from github\n",
    "# !pip install -q datasets loralib sentencepiece\n",
    "# !pip -q install bitsandbytes accelerate xformers einops\n",
    "# !pip -q install langchain\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 01:27:46.305942: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-18 01:27:46.947035: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "from LangChain_chatbot_util import *\n",
    "import os\n",
    "key = read_api_key()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models:<p>\n",
    "meta-llama/Llama-2-7b-chat-hf<p>\n",
    "lmsys/vicuna-33b-v1.3<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goetschm/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:674: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-13b-v1.3\",\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                          use_auth_token=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goetschm/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa559bf434ad4225927771fca7ec35ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-13b-v1.3\",\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             use_auth_token=True,\n",
    "                                            # load_in_8bit=True,\n",
    "                                            # load_in_4bit=True\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline for later\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 100,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "def generate(text):\n",
    "    prompt = get_prompt(text)\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=100,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs#, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        return cut_off_text(wrapped_text, \"Key\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single prompt only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "    You are an internal system assistant who is searching for keywords to search a table for.\n",
      "    You categorize information in questions into the following categories: age, \n",
      "\n",
      "    System Format:\n",
      "    START: <SORT>\n",
      "    Required Categories: (age: toddler, child, adolescent, adult), (service: Therapy, Assessment, Diagnosis, Consultation, Advocacy, Educational)\n",
      "    Additional Categories: (service type), (insurance: None, Medicaid), (language), (additional keywords)\n",
      "    END: </SORT>\n",
      "    End System Format\n",
      "\n",
      "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "    \n",
      "<</SYS>>\n",
      "\n",
      "Please find and list the age of the child, the service needed, and any keywords that would be useful for a medical provider. If you can, format it like a list and use under 10 words. Please start and end the list with the word START and END: \n",
      "\n",
      " {text}[/INST]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\\\n",
    "    You are an internal system assistant who is searching for keywords to search a table for.\n",
    "    You categorize information in questions into the following categories: age, \n",
    "\n",
    "    System Format:\n",
    "    START: <SORT>\n",
    "    Required Categories: (age: toddler, child, adolescent, adult), (service: Therapy, Assessment, Diagnosis, Consultation, Advocacy, Educational)\n",
    "    Additional Categories: (service type), (insurance: None, Medicaid), (language), (additional keywords)\n",
    "    END: </SORT>\n",
    "    End System Format\n",
    "\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "    \"\"\"\n",
    "\n",
    "instruction = \"Please find and list the age of the child, the service needed, and any keywords that would be useful for a medical provider. If you can, format it like a list and use under 10 words. Please start and end the list with the word START and END: \\n\\n {text}\"\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  START:  1. Age: 3-years-old 2. Service: Evaluation for autism 3. Additional keywords: Long wait,\n",
      "want other options  END:\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  START:  1. Age: 3-years-old 2. Service: Evaluation for autism 3. Additional keywords: Long wait,\\nwant other options  END:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My 3-year-old had a high screening for autism. Where can I go for an evaluation? The referral given by the pediatrician is a long wait and I want other options.\"\n",
    "output = llm_chain.run(text)\n",
    "\n",
    "search = parse_text(output)\n",
    "search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Age Range: Infant to 18\n",
      " years\n",
      "Support Type: Diagnosis\n",
      "Therapy Services: \n",
      "Assessment or Diagnosis Services: Autism Diagnosis,  Complex Autism Diagnosis\n",
      "Advocacy or Education Services: \n",
      "Insurance: \n",
      "Language: \n",
      "Notes: \n",
      "\n",
      "\n",
      "Age Range: 5 years and under\n",
      "Support Type: Assessment\n",
      "Therapy Services: \n",
      "Assessment or Diagnosis Services: Neuropsychological Assessment\n",
      "Advocacy or Education Services: \n",
      "Insurance: Medicaid provider\n",
      "Language: Translation may be available\n",
      "Notes: Will not conduct Autism Diagnosis unless there is medical concern (epilepsy, brain injury)\n",
      "\n",
      "\n",
      "Age Range: 18 months to 5 years\n",
      "Support Type: Therapy\n",
      "Therapy Services: Intensive therapy\n",
      "Assessment or Diagnosis Services: Autism Diagnosis\n",
      "Advocacy or Education Services: \n",
      "Insurance: Medicaid provider\n",
      "Language: Spanish speaking staff\n",
      "Notes: \n",
      "\n",
      "\n",
      "Age Range: 6 to 18 years\n",
      "Support Type: Diagnosis, Therapy\n",
      "Therapy Services: Non-intensive therapy\n",
      "Assessment or Diagnosis Services: Autism plus other Diagnoses\n",
      "Advocacy or Education Services: \n",
      "Insurance: \n",
      "Language: \n",
      "Notes: \n"
     ]
    }
   ],
   "source": [
    "from clinic_match import ClinicMatch\n",
    "\n",
    "cm = ClinicMatch(key)\n",
    "output = cm.query(search)\n",
    "for i in range(4):\n",
    "    print(\"\\n\")\n",
    "    print(output[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
