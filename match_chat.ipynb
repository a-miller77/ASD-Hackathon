{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable\n",
    "# %pip install langchain\n",
    "# %pip install huggingface_hub\n",
    "# %pip install sentence_transformers\n",
    "# %pip install faiss-cpu\n",
    "# %pip install unstructured\n",
    "# %pip install chromadb\n",
    "# %pip install Cython\n",
    "# %pip install tiktoken\n",
    "# %pip install unstructured[local-inference]\n",
    "# %pip install -q datasets loralib sentencepiece\n",
    "# %pip -q install bitsandbytes accelerate xformers einops\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate,  LLMChain\n",
    "\n",
    "from LangChain_chatbot_util import *\n",
    "\n",
    "import os\n",
    "from clinic_match import ClinicMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = read_api_key()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = key\n",
    "cm = ClinicMatch(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models:<p>\n",
    "meta-llama/Llama-2-7b-chat-hf<p>\n",
    "lmsys/vicuna-33b-v1.3<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goetschm/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:674: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61b44c8c9984efe8873090511c5b79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e57725fbc1462aa50874f2fb761b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845cf21c6b0f4c329f361d53369a7099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\",\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                          use_auth_token=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5028bf9e644547b9c6c2a7e26b3687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"t5-small\",\n",
    "# # model = AutoModelForTextClassification.from_pretrained(\"lmsys/vicuna-7b-v1.3\",\n",
    "# # model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "#                                              device_map='auto',\n",
    "#                                              torch_dtype=torch.float16,\n",
    "#                                              token=True,\n",
    "#                                             # load_in_8bit=True,\n",
    "#                                             # load_in_4bit=True\n",
    "#                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5612177c39a54bffbfc8cd2f7f3e7f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pipe = pipeline(\"text-generation\",\n",
    "#                 model=model,\n",
    "#                 tokenizer= tokenizer,\n",
    "#                 torch_dtype=torch.bfloat16,\n",
    "#                 device_map=\"auto\",\n",
    "#                 max_new_tokens = 250,\n",
    "#                 do_sample=True,\n",
    "#                 top_k=30,\n",
    "#                 num_return_sequences=1,\n",
    "#                 eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "\n",
    "pip = pipeline(\"text-generation\", model=\"lmsys/vicuna-7b-v1.3\", tokenizer=tokenizer, device_map=\"auto\",\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
    "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
    "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "def cut_off_text(text, prompt):\n",
    "    cutoff_phrase = prompt\n",
    "    index = text.find(cutoff_phrase)\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def remove_substring(string, substring):\n",
    "    return string.replace(substring, \"\")\n",
    "\n",
    "\n",
    "def generate(text, prompt):\n",
    "    prompt = prompt\n",
    "    with torch.autocast('cuda', dtype=torch.float16):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 max_new_tokens=50,\n",
    "                                 eos_token_id=tokenizer.eos_token_id,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 )\n",
    "        final_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        final_outputs = cut_off_text(final_outputs, '</s>')\n",
    "        final_outputs = remove_substring(final_outputs, prompt)\n",
    "\n",
    "    return final_outputs #, outputs\n",
    "\n",
    "def parse_text(text):\n",
    "        wrapped_text = textwrap.fill(text, width=100)\n",
    "        print(wrapped_text +'\\n\\n')\n",
    "        # return cut_off_text(wrapped_text, \"\\n\")\n",
    "        return wrapped_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```     =======================================================     =           =           =\n",
      "=           =           =     **Mental Health Professional**     **Psychologist**     **Speech-\n",
      "Language Pathologist**     **Occupational Therapist**     **Physical Therapist**\n",
      "=======================================================     START\n",
      "[P] Recommended services:                                                -age: Adolescent\n",
      "-service: Assessment\n",
      "===============================================================     Autism Assessment Clinic\n",
      "Child Development Center                                                  Community Mental Health\n",
      "Center                                              University Autism Center\n",
      "Mental Health Crisis Line                                                   ADPKD Support Group\n",
      "===============================================================     END          <SYS>\n",
      "================================================================     [INST] Recommended services for\n",
      "a 10-year-old with a high screening for autism are Assess\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "\n",
    "#### Single prompt only\n",
    "system_prompt = \"\"\"\\\n",
    "    You are an internal system key search terms assistant who is searching for keywords to search a table for.\n",
    "    You categorize information in questions into categories following system format. If nothing is found for a category, please leave the section blank. Do not fill blank categories. Additional Categories do not need to be added if blank. \n",
    "    PLEASE do not add additional keywords that are not explicitly stated in the entry.\n",
    "\n",
    "    System Format:\n",
    "\n",
    "    Required Categories: -age: (Toddler, Child, Adolescent, Adult), -service: (Therapy, Assessment, Diagnosis, Consultation, Advocacy, Educational)\n",
    "    Additional Categories: -s: (service specific), -insurance: ( ), -language: ( ), -a: (additional keywords)\n",
    "    \n",
    "    End System Format\n",
    "\n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "    \"\"\"\n",
    "\n",
    "instruction = \"Please catagorize the keywords in the request. Please start and end the list with START and END tags: \\n\\n {text}\"\n",
    "template = get_prompt(instruction, system_prompt)\n",
    "# print(template)\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0}))\n",
    "\n",
    "output = llm_chain.run(text)\n",
    "# output = generate(text, template)\n",
    "\n",
    "search = parse_text(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cm.query(search)\n",
    "for i in range(4):\n",
    "    print(\"\\n\")\n",
    "    print(output[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 18 03:44:03 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                        On | 00000000:60:00.0 Off |                    0 |\n",
      "| N/A   52C    P0               31W /  70W|  14028MiB / 15360MiB |      4%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla T4                        On | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   55C    P0               30W /  70W|  14514MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla T4                        On | 00000000:DA:00.0 Off |                    0 |\n",
      "| N/A   57C    P0               29W /  70W|  14914MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2232450      C   /usr/bin/python                           14024MiB |\n",
      "|    1   N/A  N/A   2232450      C   /usr/bin/python                           14510MiB |\n",
      "|    2   N/A  N/A   2232450      C   /usr/bin/python                           14910MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
