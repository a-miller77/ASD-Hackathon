{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama.cpp here!\n",
    "\n",
    "# You must be running this notebook as a job (this is the default case, so you're probably fine)\n",
    "# Only run this cell once. You can comment these lines out after installation.\n",
    "\n",
    "# %env CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "# %env FORCE_CMAKE=1\n",
    "# %pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf'\n",
    "llm = Llama(path, n_ctx = 512, n_gpu_layers=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_llm = Llama(path, n_ctx = 4000, n_gpu_layers=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_llm = Llama(path, n_gpu_layers=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_context(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            file_contents = file.read()\n",
    "        return '```\\n' + file_contents + '\\n```'\n",
    "    except FileNotFoundError:\n",
    "        return f'File not found: {file_path}'\n",
    "    except Exception as e:\n",
    "        return f'An error occurred: {e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextFactory():\n",
    "    _contexts = {}\n",
    "    @staticmethod\n",
    "    def get_context(file_name):\n",
    "        if ContextFactory._contexts.get(file_name, None) == None:\n",
    "            print('loading from file')\n",
    "            ContextFactory._contexts[file_name] = load_context(f'contexts/{file_name}.txt')\n",
    "        return ContextFactory._contexts.get(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMResponseToContextFactory():\n",
    "    _response_contexts = {}\n",
    "    @staticmethod\n",
    "    def get_response(context_name):\n",
    "        if LLMResponseToContextFactory._response_contexts.get(context_name, None) == None:\n",
    "            print('generating response')\n",
    "            context = context_builder(context_name, output='string')\n",
    "            prompt = llama_v2_context_prompt(context)\n",
    "            #print(f'Prompt: {prompt}')\n",
    "            response = ''\n",
    "            while response == '':\n",
    "                response = bool_llm.create_completion(prompt, repeat_penalty=1.2, temperature=0.2)['choices'][0]['text'] \n",
    "            LLMResponseToContextFactory._response_contexts[context_name] = response\n",
    "        return LLMResponseToContextFactory._response_contexts.get(context_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_model(llm_, prompt) -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #print(f'len of hist (prompt): {len(history)}')\n",
    "    msg_result = ''\n",
    "    while msg_result == '':\n",
    "        result = llm_.create_completion(prompt, repeat_penalty=1.2, temperature=0.2)\n",
    "        msg_result = result['choices'][0]['text']\n",
    "    return result['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_builder(context_name: str, output: str = 'dict') -> list[dict]:\n",
    "    context = (\n",
    "            f'[context]\\n'\n",
    "            f'{ContextFactory.get_context(context_name)}\\n'\n",
    "            f'[/context]\\n'\n",
    "         )\n",
    "    if output == 'dict': \n",
    "        response = LLMResponseToContextFactory.get_response(context_name)\n",
    "        history = [\n",
    "            {'role': 'user', \n",
    "             'content': context\n",
    "            },\n",
    "            {'role': 'assistant',\n",
    "             'content': response\n",
    "            }\n",
    "        ]\n",
    "        return history\n",
    "    elif output in ['string', 'str'] :\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_llm(context_name: str, dev_prompt: str, user_prompt: str, history=[]) -> bool:\n",
    "    \"\"\"\n",
    "    - Question model with a Yes/No form of question and return boolean of response\n",
    "    \"\"\"\n",
    "    for cntx in context_builder(context_name):\n",
    "        history.append(cntx)\n",
    "    dev_prompt = dev_prompt + '. Answer with a Yes/No'\n",
    "    prompt = llama_v2_prompt(user_prompt, dev_prompt=dev_prompt, messages=history)\n",
    "    print(prompt)\n",
    "    result = prompt_model(bool_llm, prompt)\n",
    "    print(result)\n",
    "    print('\\n')\n",
    "    #print(f'len of hist (question out): {len(history)}')\n",
    "    #print(history)\n",
    "    #print(result)\n",
    "    return 'yes' in result.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_faq(user_prompt: str) -> bool:\n",
    "    #history = [{'role':'system', 'content':'You are a ASD question classifier'}]\n",
    "    #print(f'len of hist (is_faq in): {len(history)}')\n",
    "    context_name = 'ASD_general'\n",
    "    dev_prompt = 'Keeping in mind the context, is the following question a generic question about ASD?'\n",
    "    boolean = question_llm(context_name, dev_prompt, user_prompt)\n",
    "    #print(f'len of hist (is_faq out): {len(history)}')\n",
    "    return boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_screening(user_prompt: str) -> bool:\n",
    "    #history = [{'role':'system', 'content':'You are a ASD question classifier'}]\n",
    "    #print(f'len of hist (is_screening in): {len(history)}')\n",
    "    context_name = 'ASD_screen'\n",
    "    dev_prompt = 'Keeping in mind the context, is the following question seeking to getting a child screened for ASD?'\n",
    "    response = question_llm(context_name, dev_prompt, user_prompt)\n",
    "    #print(f'len of hist (is_screening out): {len(history)}')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_request_type(prompt: str) -> str:\n",
    "    result = ''\n",
    "    if is_screening(prompt): return 'screen'\n",
    "    if is_faq(prompt): return 'faq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_faq(prompt: str, history = []):\n",
    "    #history = [{'role':'system', 'content':'You are a ASD question classifier'}]\n",
    "    \"\"\"\n",
    "    Not complete, will return a Literal[str] of the type of faq being asked in order to give the correct\n",
    "    context to the model\n",
    "    \"\"\"\n",
    "    if True: faq_type = 'general'\n",
    "    if False: faq_type = 'symptoms'\n",
    "    if False: faq_type = 'screening_diagnosis'\n",
    "    if False: faq_type = 'treatment'\n",
    "    return faq_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_faq(prompt: str, history = []):\n",
    "    #history = [{'role':'system', 'content':'You are a helpful assistant that gives simple and concise answers'}]\n",
    "    #print(f'len of hist (faq in): {len(history)}')\n",
    "    faq_type = bucket_faq(prompt)\n",
    "    context_name = f'ASD_{faq_type}'\n",
    "    \n",
    "    history.append(context_builder(context_name))\n",
    "    prompt = llama_v2_prompt(prompt, history, dev_prompt = 'Remember the context when answering questions')\n",
    "    result = prompt_model(llm, prompt)\n",
    "    #print(f'len of hist (faq out): {len(history)}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_response(prompt: str):\n",
    "    request_type = determine_request_type(prompt)\n",
    "    if request_type == 'screen': return 'begin screening process'\n",
    "    if request_type == 'faq': return answer_faq(prompt)\n",
    "    return 'I cannot help with that as it is outside the bounds of my expertise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_v2_context_prompt(context: str, sys_prompt: dict = ''):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    BOS, EOS = \"<s>\", \"</s>\"\n",
    "    DEFAULT_SYSTEM_PROMPT = f\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "    if sys_prompt == '': sys_prompt = DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": 'system',\n",
    "            \"content\": B_SYS + sys_prompt + E_SYS\n",
    "        },\n",
    "        {\n",
    "            \"role\": 'user',\n",
    "            \"CONTEXT\": (\n",
    "                f'{B_INST}Keep in mind the following context{E_INST}\\n'\n",
    "                f'{context}\\n'\n",
    "            ),\n",
    "            \"content\": 'respond if you understand'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    system_msg = messages[0]['content']\n",
    "    context_msg = messages[1]['CONTEXT']\n",
    "    messages_list = [system_msg, context_msg]\n",
    "    \n",
    "    str1 = f\"{B_INST}Remember the context when answering questions{E_INST}\"\n",
    "    str2 = 'User: \"respond if you understand\"\\n'\n",
    "    messages_list.append(str1 + str2)\n",
    "    \n",
    "    return \"\".join(messages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_v2_prompt(prompt: str, \n",
    "                    messages: list[dict],\n",
    "                    dev_prompt: str = 'Remember the context when answering questions'\n",
    "                   ):\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    BOS, EOS = \"<s>\", \"</s>\"\n",
    "    DEFAULT_SYSTEM_PROMPT = f\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "    #print(messages)\n",
    "    \n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "            }\n",
    "        ] + messages\n",
    "        \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": messages[0][\"role\"],\n",
    "            \"content\": B_SYS + messages[0][\"content\"] + E_SYS\n",
    "        },\n",
    "        {\n",
    "            \"role\": messages[1][\"role\"],\n",
    "            \"CONTEXT\": (\n",
    "                f'{B_INST}Keep in mind the following context{E_INST}\\n'\n",
    "                f'{messages[1][\"content\"]}\\n'\n",
    "            ),\n",
    "            \"content\": 'respond if you understand'\n",
    "        }\n",
    "    ] + messages[2:] + [{\n",
    "        'role':'user',\n",
    "        'content': prompt\n",
    "    }\n",
    "    ]\n",
    "    \n",
    "    system_msg = messages[0]['content']\n",
    "    context_msg = messages[1]['CONTEXT']\n",
    "    messages_list = [system_msg, context_msg]\n",
    "    for prompt in messages[1:-1:]:\n",
    "        #f\"{BOS}{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}\"\n",
    "        #str1 = f\"{B_INST}Remember the context when answering questions{E_INST}\" if prompt['role'] == 'user' else ''\n",
    "        str2 = f'{prompt[\"role\"]}: \"{(prompt[\"content\"]).strip()}\"\\n'\n",
    "        messages_list.append(str2)\n",
    "    #messages_list.append(f\"{BOS}{B_INST} {(messages[-1]['content']).strip()} {E_INST}\")\n",
    "    \n",
    "    str1 = f\"{B_INST}{dev_prompt}{E_INST}\"\n",
    "    str2 = f'{messages[-1][\"role\"]}: \"{(messages[-1][\"content\"]).strip()}\"\\n'\n",
    "    messages_list.append(str1 + str2)\n",
    "    \n",
    "    #print(messages_list)\n",
    "    \n",
    "    return \"\".join(messages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(llama_v2_prompt('test', context_builder('ASD_general')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_response('What is Autism?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def llama_v2_prompt(prompt: str, messages: list[dict]):\n",
    "#     B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "#     B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "#     BOS, EOS = \"<s>\", \"</s>\"\n",
    "#     DEFAULT_SYSTEM_PROMPT = f\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "#     if messages[0][\"role\"] != \"system\":\n",
    "#         messages = [\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "#             }\n",
    "#         ] + messages\n",
    "        \n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": messages[0][\"role\"],\n",
    "#             \"content\": B_SYS + messages[0][\"content\"] + E_SYS\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": messages[0][\"role\"],\n",
    "#             \"content\": B_SYS + messages[0][\"content\"] + E_SYS\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": messages[1][\"role\"],\n",
    "#             \"CONTEXT\": (\n",
    "#                 f'{B_INST}Keep in mind the following context{E_INST}\\n'\n",
    "#                 f'{messages[1][\"content\"]}\\n'\n",
    "#             ),\n",
    "#             \"content\": 'respond if you understand'\n",
    "#         }\n",
    "#     ] + messages[2:] + [{\n",
    "#         'role':'user',\n",
    "#         'content': prompt\n",
    "#     }\n",
    "#     ]\n",
    "    \n",
    "#     system_msg = messages[0]['content']\n",
    "#     context_msg = messages[1]['CONTEXT']\n",
    "#     messages_list = [system_msg, context_msg]\n",
    "#     for prompt in messages[1:-1:]:\n",
    "#         #f\"{BOS}{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}\"\n",
    "#         #str1 = f\"{B_INST}Remember the context when answering questions{E_INST}\" if prompt['role'] == 'user' else ''\n",
    "#         str2 = f'{prompt[\"role\"]}: \"{(prompt[\"content\"]).strip()}\"\\n'\n",
    "#         messages_list.append(str2)\n",
    "#     #messages_list.append(f\"{BOS}{B_INST} {(messages[-1]['content']).strip()} {E_INST}\")\n",
    "    \n",
    "#     str1 = f\"{B_INST}Remember the context when answering questions{E_INST}\"\n",
    "#     str2 = f'{messages[-1][\"role\"]}: \"{(messages[-1][\"content\"]).strip()}\"\\n'\n",
    "#     messages_list.append(str1 + str2)\n",
    "    \n",
    "#     return \"\".join(messages_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
